---
layout: incidencia
title: "Error de timeout en conexiones a base de datos PostgreSQL"
date: 2026-01-14 10:30:00 +0100
cliente: "acme-corp"
estado: "resuelto"
prioridad: "alta"
tecnologias:
  - "PostgreSQL"
  - "AWS RDS"
  - "Connection Pool"
descripcion_breve: "Timeouts intermitentes en conexiones a PostgreSQL causando errores 500 en la aplicaci√≥n web"
impacto: "Aproximadamente 15% de las peticiones fallaban, afectando a usuarios durante el checkout"
tiempo_resolucion: "2 horas 15 minutos"
author: tecnico1
toc: true
relacionadas: []
lecciones_aprendidas: |
  - Monitorear activamente el uso del connection pool
  - Configurar alertas proactivas cuando el pool alcance 80% de uso
  - Revisar queries lentas que mantienen conexiones abiertas
---

## üìã Descripci√≥n del Problema

**Contexto:**
El d√≠a 14/01/2026 a las 10:30h, el sistema de monitoreo detect√≥ un incremento significativo en errores HTTP 500 en la aplicaci√≥n web de Acme Corp. Los usuarios reportaron imposibilidad para completar compras.

**S√≠ntomas observados:**
- Errores intermitentes "Connection timeout" en logs de aplicaci√≥n
- Tiempo de respuesta API incrementado de 200ms a 3000ms
- Tasa de error HTTP 500: 15% del tr√°fico total
- Dashboard de monitoreo mostraba connection pool al 100%

**Fecha de detecci√≥n:** 14/01/2026 10:30:00

**Usuarios/Sistemas afectados:**
- Aplicaci√≥n web principal (checkout y b√∫squeda de productos)
- Aproximadamente 500 usuarios concurrentes afectados
- P√©rdida estimada: 50 transacciones durante el incidente

---

## üîç Diagn√≥stico y Causa Ra√≠z

**Proceso de investigaci√≥n:**

1. **Verificaci√≥n de estado de base de datos RDS:**
   - CPU: 45% (normal)
   - Conexiones activas: 200/200 (m√°ximo alcanzado)
   - Memoria: 70% (normal)
   
2. **An√°lisis de logs de aplicaci√≥n:**
   ```
   [ERROR] 2026-01-14T10:32:15 - Connection timeout after 5000ms
   [ERROR] 2026-01-14T10:32:18 - Pool exhausted, waiting for available connection
   [ERROR] 2026-01-14T10:32:20 - Could not acquire connection from pool
   ```

3. **Revisi√≥n de queries activas:**
   Detectadas m√∫ltiples queries de b√∫squeda sin cerrar conexi√≥n correctamente

**Causa ra√≠z identificada:**

El problema fue causado por un bug en el m√≥dulo de b√∫squeda introducido en el despliegue de la versi√≥n 2.5.3 del d√≠a anterior. Las b√∫squedas complejas con m√∫ltiples filtros no liberaban las conexiones del pool correctamente, causando acumulaci√≥n de conexiones "colgadas".

**Evidencias:**
```sql
-- Query para ver conexiones activas
SELECT 
    pid,
    usename,
    application_name,
    client_addr,
    state,
    query_start,
    state_change,
    query
FROM pg_stat_activity
WHERE state != 'idle'
ORDER BY query_start;

-- Resultado: 180 conexiones en estado "idle in transaction" 
-- desde hace m√°s de 5 minutos
```

---

## ‚úÖ Soluci√≥n Implementada

**Pasos de resoluci√≥n:**

1. **Acci√≥n inmediata - Liberar conexiones colgadas:**
   ```sql
   -- Terminar conexiones idle en transacci√≥n > 5 minutos
   SELECT pg_terminate_backend(pid)
   FROM pg_stat_activity
   WHERE state = 'idle in transaction'
   AND state_change < now() - interval '5 minutes';
   
   -- Resultado: 175 conexiones terminadas
   ```
   **Tiempo:** 10:35h - Pool liberado, servicio restaurado parcialmente

2. **Rollback de c√≥digo problem√°tico:**
   ```bash
   # Rollback a versi√≥n anterior estable
   cd /opt/acme-app
   git checkout v2.5.2
   pm2 restart all
   
   # Verificar que el servicio responde correctamente
   curl -I https://api.acmecorp.com/health
   ```
   **Tiempo:** 10:45h - Servicio completamente restaurado

3. **Incremento temporal del connection pool:**
   ```javascript
   // config/database.js - Configuraci√≥n temporal
   pool: {
     max: 300,        // Aumentado de 200
     min: 20,
     idle: 10000,
     acquire: 30000,
     evict: 10000
   }
   ```
   **Tiempo:** 11:00h - Mayor capacidad disponible

**Configuraci√≥n modificada:**
- `max_connections` PostgreSQL: 200 ‚Üí 300
- `pool.max` aplicaci√≥n: 200 ‚Üí 300
- `pool.idle` aplicaci√≥n: 30000ms ‚Üí 10000ms (liberar conexiones idle m√°s r√°pido)

---

## üéØ Resultado Final

**Estado actual:**
- ‚úÖ Servicio completamente restaurado a las 11:15h
- ‚úÖ Tasa de error < 0.1% (nivel normal)
- ‚úÖ Tiempo de respuesta API: 180ms promedio
- ‚úÖ Connection pool operando al 60%

**Validaci√≥n:**
- ‚úÖ Load test con 1000 usuarios concurrentes - OK
- ‚úÖ Verificaci√≥n de b√∫squedas complejas - OK
- ‚úÖ Monitoreo de conexiones durante 2 horas - Estable

**Fecha de resoluci√≥n:** 14/01/2026 12:45:00

---

## üìö Lecciones Aprendidas

**Conclusiones:**
- El c√≥digo del m√≥dulo de b√∫squeda no implementaba correctamente el patr√≥n try-finally para cerrar conexiones
- Faltaban pruebas de integraci√≥n que verificaran la liberaci√≥n de conexiones
- El monitoreo actual no alertaba proactivamente sobre pool exhaustion

**Mejoras propuestas:**
- [x] Implementar alerta cuando connection pool > 80%
- [x] Code review para verificar cierre correcto de conexiones
- [ ] A√±adir pruebas automatizadas de connection pooling
- [ ] Implementar circuit breaker para prevenir cascadas de fallos
- [ ] Documentar patrones correctos de manejo de conexiones

**Documentaci√≥n actualizada:**
- [x] Runbook de respuesta a connection pool exhaustion
- [x] Playbook de rollback de despliegues
- [ ] Gu√≠a de desarrollo: Manejo de conexiones DB

---

## ü§ñ Notas del Agente IA

**Contexto para IA:**
```json
{
  "tipo_problema": "database_connection_pool_exhaustion",
  "severidad": "alta",
  "tiempo_deteccion_minutos": 5,
  "tiempo_resolucion_minutos": 135,
  "automatizable": true,
  "keywords": [
    "postgresql", 
    "connection pool", 
    "timeout", 
    "idle in transaction",
    "resource exhaustion"
  ],
  "patron_similar": "INC-2025-03-15-db-connections",
  "metricas": {
    "conexiones_activas_pico": 200,
    "conexiones_idle_transaction": 175,
    "error_rate_pico": "15%",
    "usuarios_afectados": 500,
    "transacciones_perdidas": 50
  }
}
```

**Patrones detectados:**
- **Patr√≥n:** Agotamiento de connection pool tras despliegue
- **Frecuencia:** 2¬™ vez en 6 meses (anterior: marzo 2025)
- **Indicador temprano:** Incremento gradual de conexiones "idle in transaction"

**Recomendaciones para prevenci√≥n:**
- Implementar timeout autom√°tico para conexiones idle in transaction (< 30s)
- Pre-deployment check: validar que nuevos cambios no introducen leaks de conexiones
- Monitoreo continuo de pg_stat_activity con alertas tempranas
- Considerar usar PgBouncer como connection pooler externo

**Automatizaci√≥n posible:**
```python
# Script de detecci√≥n autom√°tica
def check_connection_pool_health():
    idle_connections = query("SELECT count(*) FROM pg_stat_activity 
                             WHERE state = 'idle in transaction' 
                             AND state_change < now() - interval '2 minutes'")
    
    if idle_connections > 50:
        alert("High number of idle in transaction connections")
        # Auto-remediation: kill connections > 5 minutes
        query("SELECT pg_terminate_backend(pid) 
               FROM pg_stat_activity 
               WHERE state = 'idle in transaction' 
               AND state_change < now() - interval '5 minutes'")
```

**Relaci√≥n con incidencias previas:**
Esta incidencia es similar a INC-2025-03-15 donde tambi√©n hubo problemas con connection pool, pero en ese caso fue por un query lento, no por leak de conexiones.

---

## üìé Archivos Adjuntos

- [Logs completos de la incidencia](../assets/logs/2026-01-14-db-timeout.log)
- [Gr√°ficas de monitoreo](../assets/images/incidencias/2026-01-14-graphs.png)
- [An√°lisis de queries lentas](../assets/docs/2026-01-14-slow-queries.pdf)
